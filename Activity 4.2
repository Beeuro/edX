-Street Names
# Load the Jupyter notebook to add examples of .loc[] and .iloc[]
import nbformat

# Load the notebook
with open('/mnt/data/loc_iloc.ipynb', 'r') as f:
    nb = nbformat.read(f, as_version=4)

# Function to add a new code cell
def add_code_cell(notebook, code):
    new_cell = nbformat.v4.new_code_cell(code)
    notebook.cells.append(new_cell)

# Examples of .loc[] and .iloc[]
code_examples = """
# Examples of using .loc[] and .iloc[] with the DataFrame

## Using .iloc[] to select rows by integer index:
# Select the first row
first_row = df.iloc[0]

# Select the first three rows
first_three_rows = df.iloc[:3]

## Using .loc[] to select rows by index label:
# Assuming the DataFrame's index is set to 'STREET NAME ID', select the street with ID 1400342
# First, set 'STREET NAME ID' as the index
df.set_index('STREET NAME ID', inplace=True)

# Now select the street with ID 1400342
street_1400342 = df.loc[1400342]

# Reset index for further operations
df.reset_index(inplace=True)

print('First row using iloc:\\n', first_row)
print('\\nFirst three rows using iloc:\\n', first_three_rows)
print('\\nStreet with ID 1400342 using loc:\\n', street_1400342)
"""

# Add the code example to the notebook
add_code_cell(nb, code_examples)

# Save the updated notebook
updated_notebook_path = '/mnt/data/loc_iloc_updated.ipynb'
with open(updated_notebook_path, 'w') as f:
    nbformat.write(nb, f)

updated_notebook_path

-Movie Scores
import pandas as pd

# Load the CSV file
df = pd.read_csv('/mnt/data/movie_scores.csv')

# Display the dataframe
df.head()

# Create a new table with only the film and the columns related to IMDb
imdb_data = df[['FILM', 'IMDB', 'IMDB_norm', 'IMDB_norm_round', 'IMDB_user_vote_count']]

# Display the new table
imdb_data.head()

# Filter out movies with IMDb score >= 7 and remove norm ratings
good_movies = imdb_data[imdb_data['IMDB'] >= 7].drop(['IMDB_norm', 'IMDB_norm_round'], axis=1)

# Display the filtered dataset
good_movies.head()

# Filter for less popular movies with under 20K votes
less_popular_good_movies = good_movies[good_movies['IMDB_user_vote_count'] < 20000]

# Display the filtered dataset
less_popular_good_movies

-Employment
import pandas as pd

# Load the data
df = pd.read_csv('/path/to/donors2021_unclean.csv')

# Standardize text entries
df['Employer'] = df['Employer'].replace(['UNEMPLOYED', 'NOT EMPLOYED'], 'NOT EMPLOYED')

# Correct zip code formatting
df['Zip'] = df['Zip'].astype(str).str.pad(width=5, side='left', fillchar='0').str[:5]

# Handle missing values in 'Memo_CD'
df.drop(columns=['Memo_CD'], inplace=True)  # Example of dropping the column

# Remove duplicates
df = df.drop_duplicates()

# Save the cleaned dataset
df.to_csv('/path/to/cleaned_donors2021.csv', index=False)

import pandas as pd

# Load the datasets
donors2021 = pd.read_csv('/mnt/data/donors2021.csv')
donors2021_unclean = pd.read_csv('/mnt/data/donors2021_unclean.csv')

# Display the first few rows of each dataset for initial assessment
donors2021.head(), donors2021_unclean.head()

# Step 1: Load the Data
# We already have the dataframe loaded as `donors2021_unclean`, so we'll start from the next step.

# Step 2: Standardize Text Entries
donors2021_unclean['Employer'] = donors2021_unclean['Employer'].replace(['UNEMPLOYED', 'NOT EMPLOYED'], 'NOT EMPLOYED')

# Display the first few rows to verify the changes
donors2021_unclean.head()

# Step 3: Correct Zip Code Formatting
# Convert Zip codes to string and ensure they are 5 characters long
donors2021_unclean['Zip'] = donors2021_unclean['Zip'].astype(str).str[:5]

# Display the first few rows to verify the changes
donors2021_unclean.head()

# Step 4: Handle Missing Values in 'Memo_CD'
# Remove the 'Memo_CD' column as it's mostly NaN and may not be relevant
donors2021_unclean.drop(columns=['Memo_CD'], inplace=True)

# Display the first few rows to verify the changes
donors2021_unclean.head()

# Step 5: Remove Duplicates
# Remove any duplicate rows in the dataset
donors2021_unclean.drop_duplicates(inplace=True)

# Display the shape of the dataframe to verify changes and ensure duplicates were removed
donors2021_unclean.shape

-Appliance Data
import pandas as pd

# Load the dataset using UTF-8 encoding
df = pd.read_csv('/mnt/data/dga_lpg.csv', encoding='utf-8')

# Display the DataFrame
df.head()

# Reduce the DataFrame to only include columns with English descriptions
df_english = df[['Part', 'Type', 'Brand', 'Model', 'Other Information', 'Place of Manufacture', 'Applicant', 'Telephone Number', 'Approval Expiry Date']]

# Get a count of the rows within the DataFrame to determine if there are any null values
null_counts = df_english.isnull().sum()

# Display the count of null values for each column
null_counts

# Drop the rows that contain null values
df_cleaned = df_english.dropna()

# Since the task requires looking into the "Applicant" column for any similar values,
# Let's first inspect the unique values to identify similar ones that need consolidation
unique_applicants = df_cleaned['Applicant'].unique()

# Display unique applicants to identify similar ones
unique_applicants[:10]  # Display a subset to keep the output manageable

# Create DataFrames for specific places of manufacture
df_italy = df_cleaned[df_cleaned['Place of Manufacture'] == 'Italy']
df_china = df_cleaned[df_cleaned['Place of Manufacture'] == 'The People\'s Republic of China']

# Display the first few rows of each DataFrame to ensure correct filtering
(df_italy.head(), df_china.head())

-Pandas
# Renaming mistyped columns
fires_df.rename(columns={"Aid Given or Received Code ": "Aid Given or Received Code", 
                         "Propery Loss": "Property Loss"}, inplace=True)

# Reducing to specific columns as per the instructions
required_columns = [
    'Reporting Year', 'Fire Department Name', 'Incident date', 'Incident Type',
    'Aid Given or Received Code', 'Aid Given or Received', 'Number of Alarms', 
    'Alarm Date and Time', 'Arrival Date and Time', 'Last Unit Cleared Date and Time', 
    'Actions Taken 1', 'Actions Taken 2', 'Actions Taken 3', 'Property Value', 
    'Property Loss', 'Contents Value', 'Contents Loss', 'Fire Service Deaths', 
    'Fire Service Injuries', 'Other Fire Deaths', 'Other Fire Injuries', 'Property Use', 
    'Incident Street Address', 'Incident Apartment Number', 'Incident City', 'Incident Zip Code'
]
reduced_df = fires_df[required_columns]

# Filling missing values as per instructions
reduced_df.fillna({
    "Actions Taken 1": '',
    "Actions Taken 2": '',
    "Actions Taken 3": '',
    "Incident Apartment Number": '',
    "Other Fire Deaths": 0,
    "Other Fire Injuries": 0,
    "Property Loss": 0,
    "Contents Loss": 0
}, inplace=True)

# Removing remaining rows with missing data
cleaned_df = reduced_df.dropna()

# Filtering for incidents with Property or Contents Loss
loss_df = cleaned_df[(cleaned_df['Property Loss'] > 0) | (cleaned_df['Contents Loss'] > 0)]

# Displaying the shape of the original, reduced, cleaned, and loss DataFrame to understand the effect of these operations
fires_df.shape, reduced_df.shape, cleaned_df.shape, loss_df.shape

-Fire Department
import pandas as pd

# Load the CSV data into a DataFrame
df = pd.read_csv('/mnt/data/CT_fires_2015.csv')

# Display the first few rows of the DataFrame
df.head()

# Load the Jupyter notebook to inspect its content
import nbformat

# Reading the notebook
with open('/mnt/data/groupby.ipynb', 'r', encoding='utf-8') as f:
    nb = nbformat.read(f, as_version=4)

# Extracting and displaying the titles of cells to understand the analysis or steps suggested
cell_titles = []
for cell in nb['cells']:
    if cell['cell_type'] == 'markdown':
        cell_text = cell['source']
        if cell_text.startswith('#'):
            # Assuming titles are marked with '#' at the start
            cell_titles.append(cell_text.strip())

cell_titles

# Required libraries have been imported previously (Pandas)

# Convert 'Incident date' to datetime format for easier manipulation
df['Incident date'] = pd.to_datetime(df['Incident date'])

# 1. Incident Counts by Fire Department
incident_counts_fd = df.groupby('Fire Department Name')['Incident Number'].count().sort_values(ascending=False)

# 2. Incident Types Distribution
incident_types_distribution = df['Incident Type'].value_counts()

# 3. Incidents by Month
# Extracting month from the 'Incident date'
df['Incident Month'] = df['Incident date'].dt.month
incidents_by_month = df.groupby('Incident Month')['Incident Number'].count().sort_index()

# 4. Property Use and Incidents
property_use_incidents = df.groupby('Property Use')['Incident Number'].count().sort_values(ascending=False)

(incident_counts_fd.head(), incident_types_distribution.head(), incidents_by_month, property_use_incidents.head())

-Census Data Scripted
# Defining file paths for the CSV exports
totals_csv_path = '/mnt/data/census_totals_2016-2019.csv'
averages_csv_path = '/mnt/data/census_averages_2016-2019.csv'

# Exporting the DataFrames to CSV
totals.to_csv(totals_csv_path, index=False)
averages.to_csv(averages_csv_path, index=False)

# Returning the paths for download
totals_csv_path, averages_csv_path

-Tax Sorting and Analysis
import pandas as pd

# Load the VT_tax_statistics.csv file to check its contents
vt_tax_stats = pd.read_csv('/mnt/data/VT_tax_statistics.csv')

# Display the first few rows to understand its structure and type of data it contains
vt_tax_stats.head()

# Load the sorting_data.ipynb to check its contents
import nbformat

with open('/mnt/data/sorting_data.ipynb') as ipynb_file:
    ipynb_contents = nbformat.read(ipynb_file, as_version=4)

# Extracting cells containing markdown and code to understand the topics covered
markdown_cells = [cell for cell in ipynb_contents.cells if cell.cell_type == 'markdown']
code_cells = [cell for cell in ipynb_contents.cells if cell.cell_type == 'code']

# Summarize the contents based on the first few markdown and code cells
markdown_summary = [cell.source for cell in markdown_cells[:3]]
code_summary = [cell.source for cell in code_cells[:3]]

(markdown_summary, code_summary)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

csv_path = "VT_tax_statistics.csv"  # Adjust path as needed
tax_data = pd.read_csv(csv_path)

tax_data.info()  # Get an overview of the DataFrame, including data types and non-null counts
tax_data.isnull().sum()  # Check for missing values
# Depending on the analysis, decide how to handle missing values, e.g., drop or fill them

sorted_tax_data = tax_data.sort_values(by="Meals", ascending=False)

# Example: Summary statistics
tax_data.describe()

# Example: Trend analysis
tax_data[['Meals', 'Past Meals']].plot(kind='line')

# Example: Histogram of Meals Tax Collections
plt.figure(figsize=(10, 6))
sns.histplot(tax_data['Meals'], kde=True)
plt.title('Distribution of Meals Tax Collections')
plt.xlabel('Meals Tax Collections')
plt.ylabel('Frequency')
plt.show()

# Example: Comparison of Current vs. Past Meals Collections
tax_data[['Town', 'Meals', 'Past Meals']].set_index('Town').plot(kind='bar', figsize=(14, 8))
plt.title('Current vs. Past Meals Tax Collections by Town')
plt.xlabel('Town')
plt.ylabel('Tax Collections')
plt.legend(['Current Meals', 'Past Meals'])
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

-Airport Utility
# Create a new DataFrame that only includes "Electricity" utility for "Tenant" owner
electricity_tenant_df = df[(df['Utility'] == 'Electricity') & (df['Owner'] == 'Tenant')]

# Sort the DataFrame based on the level of consumption, from most to least
sorted_electricity_tenant_df = electricity_tenant_df.sort_values(by='Usage', ascending=False)

# Reset the index for the DataFrame so that the index is in order
sorted_electricity_tenant_df.reset_index(drop=True, inplace=True)

# Print out the details of the worst month to the screen
sorted_electricity_tenant_df.head(1)