  # Municpal analysis
## Descriptive statistics for numerical data
descriptive_stats = data.describe(include=[float, int])
descriptive_stats

## Grouping
import matplotlib.pyplot as plt

## Plotting the histogram for street name id
plt.figure(figsize=(10, 6))
plt.hist(data['Street Name ID'], bins=50, color='skyblue', edgecolor='black')
plt.title('distribution of street name id')
plt.xlabel('Street Name ID')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

## Grouping by POSTAL COMMUNITY and counting streets
grouped_data = data.groupby('POSTAL COMMUNITY').size().reset_index(name='STREET COUNT')

## Sorting the grouped data by street count in descending order to see the top communities
grouped_data_sorted = grouped_data.sort_values(by='STREET COUNT', ascending=False)

grouped_data_sorted.head()

## Data visualization complete

  # IMDB
import pandas as pd

## Load the CSV file
file_path = '/mnt/data/movie_scores.CSV'
data = pd.read_CSV(file_path)

## Display the first few rows of the dataframe to understand its structure
data.head()

## Analysis
# Create a new table with only the film and IMDb related columns
imdb_data = data[['FILM', 'IMDB', 'IMDB_norm', 'IMDB_norm_round', 'IMDB_user_vote_count']]

## Filter out only good movies with IMDb score >= 7 and remove norm ratings
good_movies = imdb_data[imdb_data['IMDB'] >= 7].drop(columns=['IMDB_norm', 'IMDB_norm_round'])

## Find less popular movies with under 20K votes
less_popular_good_movies = good_movies[good_movies['IMDB_user_vote_count'] < 20000]

## Export the file to a spreadsheet, excluding the index
output_path = '/mnt/data/less_popular_good_movies.CSV'
less_popular_good_movies.to_csv(output_path, index=false)

output_path

  # Donor list
import pandas as pd

## Load the first CSV file to check its structure and contents
donors2021_df = pd.read_CSV('/mnt/data/donors2021.CSV')

## Display the first few rows of the dataframe
donors2021_df.head()

## Cleanup
## Load the unclean CSV file to check its structure and contents
donors2021_unclean_df = pd.read_CSV('/mnt/data/donors2021_unclean.CSV')

## Display the first few rows of the dataframe
donors2021_unclean_df.head()

## Analysis
## Load the Jupyter notebook to check the cleaning operations specified
with open('/mnt/data/cleaning_data.ipynb', 'r') as file:
    notebook_content = file.read()

## Since the notebook is a JSON file, let's parse it to extract the code cells
import json

notebook_json = json.loads(notebook_content)
code_cells = []

## Extract code from cells
for cell in notebook_json['cells']:
    if cell['cell_type'] == 'code':
        # Extract the source code from each code cell
        code = ''.join(cell['source'])
        code_cells.append(code)

## Since directly displaying all code cells might be too much, let's show just the number of code cells and some examples
len(code_cells), code_cells[:2]  # Displaying the count and first two code cells as examples

  #Hong Kong Municpal
## Data Frame
import pandas as pd

## Load the CSV file
df = pd.read_CSV('/mnt/data/dga_lpg.CSV', encoding='utf-8')

## Display the initial dataframe
df.head()

## Reduction
# Reduce the dataframe to only the columns in English
df_english = df[['Part', 'Type', 'Brand', 'Model', 'Other Information', 'Place of Manufacture', 'Applicant', 'Telephone Number', 'Approval Expiry Date']]

## Display the reduced dataframe
df_english.head()

## Wrangle
## Get a count of rows within the DataFrame to determine if there are any null values
null_counts = df_english.isnull().sum()

## Display the count of null values for each column
null_counts

## Analysis
## Drop the rows that contain null values
df_cleaned = df_english.dropna()

## Check the shape of the dataframe after dropping null values to confirm
df_cleaned.shape

## Origin
# Before replacing, let's check the unique values in the "Applicant" column to identify similar values
applicant_unique = df_cleaned['Applicant'].unique()

## Since there can be a large number of unique applicants, display the first 20 to get a sense
applicant_unique[:20]

## Dataframe
## Creating DataFrames for specific "Place of Manufacture"

## Dataframe for appliances manufactured in Italy
df_italy = df_cleaned[df_cleaned['Place of Manufacture'] == 'Italy']

## Dataframe for appliances manufactured in The People's Republic of China
df_china = df_cleaned[df_cleaned['Place of Manufacture'] == 'The People\'s Republic of China']

## Display the shape of these dataframes as a quick check before displaying them
df_italy.shape, df_china.shape

## Outcome
## Display the first few entries from the Italy and China DataFrames
df_italy.head(), df_china.head()

Data visualization complete

  #Pandas
import pandas as pd

## Load the CSV file to examine its contents
df_fires = pd.read_CSV('/mnt/data/CT_fires_2015.CSV')

## Display the first few rows of the dataframe to understand its structure
df_fires.head()

## Recap
import nbformat

## Load the jupyter notebook to examine its contents
with open('/mnt/data/pandas_recap.ipynb') as f:
    nb = nbformat.read(f, as_version=4)

## Extract and display the text from markdown cells and code cell comments to understand the instructions
instructions = []

for cell in nb['cells']:
    if cell['cell_type'] == 'markdown':
        instructions.append(cell['source'])
    elif cell['cell_type'] == 'code':
        # Extract comments from code cells
        comments = [line for line in cell['source'].split('\n') if line.strip().startswith('#')]
        instructions.extend(comments)

instructions[:10]  # display the first 10 instructions for brevity

  # Fire Department
## Analysis
## Load the Jupyter notebook to understand the analysis requirements
import nbformat

## Reading the notebook
notebook_path = '/mnt/data/groupby.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as nb_file:
    nb_content = nbformat.read(nb_file, as_version=4)

## Extracting and displaying the cells' content to understand the analysis task
analysis_instructions = [cell['source'] for cell in nb_content.cells if cell.cell_type == 'code']
analysis_instructions

##CSV
import pandas as pd

## Load the CSV file
csv_path = '/mnt/data/CT_fires_2015.csv'
fires_df = pd.read_csv(csv_path, low_memory=False)

## Display the first five rows to understand the data structure
fires_df.head()

## Dataframe
## There doesn't seem to be a column explicitly named "property Loss" in the displayed data
## We will proceed with selecting the specified columns, assuming they are correctly named in the provided instructions

## It's also possible that some specified columns might not exactly match the dataframe due to abbreviations or errors in the instructions
## We will select the columns that are clearly intended based on the instruction and the dataframe's first few rows

selected_columns = [
    'Fire Department Name', 'Incident date', 'Incident Type Code', 'Incident Type',
    ## Assuming 'Alarm Date and Time', 'Arrival Date and Time', 'Last Unit Cleared Date and Time'
    ## may refer to columns not displayed in the initial rows.
    'Property Use', # Assuming this might be intended as 'Property Loss'
    'Contents Loss', 'Fire Service Deaths', 'Fire Service Injuries',
    'Other Fire Deaths', 'Other Fire Injuries', 'Incident City', 'Incident Zip Code'
    ## 'Contents Loss' and some other columns like 'Fire Service Deaths' are not visible in the first five rows
    ## Assuming they are part of the dataset based on the notebook instructions
]

## Check which of the selected columns are actually in the dataframe
available_columns = [col for col in selected_columns if col in fires_df.columns]

## Update dataframe to include only the selected (and available) columns
filtered_df = fires_df[available_columns]

## Fill na/nan values in numerical columns with 0 (where applicable)
numerical_columns = filtered_df.select_dtypes(include=['number']).columns
filtered_df[numerical_columns] = filtered_df[numerical_columns].fillna(0)

## Convert 'Incident date' to datetime
filtered_df['Incident date'] = pd.to_datetime(filtered_df['Incident date'])

## Display the updated DataFrame's first few rows to confirm changes
filtered_df.head(), filtered_df.dtype

## Data visualization complete

  # Census
## CSV
import pandas as pd

## Load the census data
census_data_path = '/mnt/data/census_data_2016-2019.csv'
census_data = pd.read_csv(census_data_path)

## Display the first few rows of the dataframe to understand its structure
census_data.head()

## Analysis
# Creating the totals DataFrame
totals_columns = ['Population', 'Employed Civilians', 'Unemployed Civilians', 'People in the Military', 'Poverty Count']
totals_df = census_data.groupby(['Year', 'State'])[totals_columns].sum().reset_index()

## Creating the averages DataFrame
averages_columns = ['Median Age', 'Household Income', 'Per Capita Income']
averages_df = census_data.groupby(['Year', 'State'])[averages_columns].mean().reset_index()

## Renaming columns to reflect the calculations
totals_df.rename(columns={
    'Population': 'Total Population',
    'Employed Civilians': 'Total Employed Civilians',
    'Unemployed Civilians': 'Total Unemployed Civilians',
    'People in the Military': 'Total People in the Military',
    'Poverty Count': 'Total Poverty Count'
}, inplace=true)

averages_df.rename(columns={
    'Median Age': 'Average Median Age',
    'Household Income': 'Average Household Income',
    'Per Capita Income': 'Average Per Capita Income'
}, inplace=True)

## File paths for the exported CSVs
totals_CSV_path = '/mnt/data/totals_by_year_and_state.CSV'
averages_CSV_path = '/mnt/data/averages_by_year_and_state.CSV'

## Exporting the dataframes to CSVs
totals_df.to_CSV(totals_CSV_path, index=false)
averages_df.to_CSV(averages_CSV_path, index=false)

(totals_CSV_path, averages_CSV_path)

## Data visualization complete

  # San Francisco Municpal
## Utility
import pandas as pd

## Load the CSV file into a dataframe
df = pd.read.CSV('/mnt/data/SFO_Airport_Utility_Consumption.CSV')

## Display the DataFrame
df

## List unique values in the "Utility" column
unique_utilities = df['Utility'].unique()

unique_utilities

## Create a new dataframe for the "Gas" utility with "Owner" being "Tenant"
gas_tenant_df = df[(df['Utility'] == 'Gas') & (df['Owner'] == 'Tenant')]

## Sort the dataframe based on the level of consumption, from most to least
sorted_gas_tenant_df = gas_tenant_df.sort_values(by='Usage', ascending=false)

## Reset the index for the dataframe
sorted_gas_tenant_df.reset_index(drop=true, inplace=true)

## Display the sorted dataframe
sorted_gas_tenant_df

## Gas
# Correcting the date conversion process
gas_tenant_df['Date'] = pd.to_datetime(gas_tenant_df['Year'].astype(str) + '-' + gas_tenant_df['Month Number'].astype(str) + '-01')

## Replotting the time series for Gas consumption by tenants
plt.figure(figsize=(14, 7))
sns.lineplot(x='Date', y='Usage', data=gas_tenant_df, marker='o')
plt.title('Gas Consumption by Tenants Over Time')
plt.xlabel('Date')
plt.ylabel('Gas Consumption (Therms)')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

## Save the corrected plot to a file
plt.savefig('/mnt/data/gas_consumption_by_tenants_over_time_corrected.png')
plt.show()

# Path to the corrected plot
corrected_plot_path = '/mnt/data/gas_consumption_by_tenants_over_time_corrected.png'
corrected_plot_path

Data visualization complete
